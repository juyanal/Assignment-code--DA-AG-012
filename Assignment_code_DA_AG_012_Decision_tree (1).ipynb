{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:** **What is a Decision Tree, and how does it work in the context of**\n",
        "**classification**.\n",
        "\n",
        "Ans : A Decision Tree is a type of supervised machine learning algorithm that is\n",
        "mainly used for classification and regression tasks. In classification, it helps predict\n",
        "the category or class of a data point based on input features.\n",
        "The structure of a decision tree is similar to a flowchart. It starts at the top with a root\n",
        "node, which represents the entire dataset. From there, the data is split into branches\n",
        "using decision rules based on feature values. Each split leads to a new internal node\n",
        "or a leaf node, which holds the final prediction.\n",
        "\n",
        "How it works:\n",
        "1. The algorithm looks for the feature that best divides the data into classes.\n",
        "2. It uses metrics like Gini Impurity or Information Gain to determine the best splits.\n",
        "3. This process continues recursively, creating new branches until stopping criteria\n",
        "are met (like maximum depth or pure leaves).\n",
        "Example: Suppose we want to classify whether a person will buy a product or not\n",
        "based on their age and income. The tree might first split by age (>30 or <=30), then\n",
        "by income level.\n",
        "\n",
        "Conclusion: Decision Trees are easy to understand and interpret. They mimic human decision-making, making them popular in business and educational settings.\n"
      ],
      "metadata": {
        "id": "V7VuTvaqMKLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.**\n",
        "\n",
        "**How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "Ans : In a decision tree, Gini Impurity and Entropy are used to measure how\n",
        "mixed the classes are in a dataset. These help the algorithm decide\n",
        "where to split the data for the best classification.\n",
        "\n",
        "1. Gini Impurity:\n",
        "Measures the probability of wrongly classifying a randomly chosen\n",
        "element.\n",
        "\n",
        "Formula: (Gini = 1 - p_i^2) where (p_i) is the probability of\n",
        "class (i). A Gini value of 0 means perfect classification.\n",
        "\n",
        "2. Entropy:\n",
        "\n",
        "● Comes from information theory. Measures disorder or uncertainty.\n",
        "\n",
        "● Formula: (Entropy = -p_i _2(p_i))\n",
        "\n",
        "● Entropy is highest when classes are equally mixed.\n",
        "\n",
        "Impact on Splits:\n",
        "\n",
        "● The decision tree selects the feature and threshold that results in\n",
        "the greatest reduction in impurity (either Gini or Entropy).\n",
        "\n",
        "● This helps create pure child nodes where samples mostly belong\n",
        "to one class.\n",
        "\n",
        "Example: If a node has 10 class A and 10 class B samples, impurity is\n",
        "high. A good split will create child nodes like one with 9A, 1B and\n",
        "another with 1A, 9B."
      ],
      "metadata": {
        "id": "d9DSK0zSMWmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**: What is the difference between Pre-Pruning and Post-Pruning in\n",
        "Decision Trees?Give one practical advantage of using each.\n",
        "\n",
        "Ans: Pre-Pruning (Early Stopping):\n",
        "\n",
        "● Stops the tree from growing too large during training.\n",
        "\n",
        "● It uses rules like max_depth, min_samples_split, or min_samples_leaf to\n",
        "limit growth.\n",
        "\n",
        "● Prevents overfitting by simplifying the tree early.\n",
        "\n",
        "Advantage:\n",
        "\n",
        "● Faster training time since it avoids building a large tree unnecessarily.\n",
        "Post-Pruning:\n",
        "\n",
        "● First builds a full tree, then removes branches that do not improve accuracy.\n",
        "\n",
        "● Also called cost-complexity pruning.\n",
        "\n",
        "● Advantage:\n",
        "\n",
        "Leads to a more accurate and generalized model, since pruning\n",
        "is done after seeing the full data. Conclusion: Both methods help avoid\n",
        "overfitting. Pre-pruning saves time, while post pruning improves model\n",
        "performance.\n",
        "\n",
        "Conclusion: Both methods help avoid overfitting. Pre-pruning saves time, while post\n",
        "pruning improves model performance.\n"
      ],
      "metadata": {
        "id": "i1ZcXnlKNDDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** **What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "\n",
        "Ans: Information Gain is a metric used to choose the feature that best splits the dataset in a Decision Tree.\n",
        "It measures the reduction in entropy after a dataset is split based on a feature. The idea is that a good split gives us more “pure” groups.\n",
        "\n",
        "Formula: (IG = Entropy(parent) - Entropy(child))\n",
        "\n",
        "Why it’s important:\n",
        "\n",
        "● A higher information gain means better separation between classes.\n",
        "\n",
        "● The tree selects the feature with the highest information gain at each step.\n",
        "\n",
        "Example:\n",
        "If we split data by the feature “Age > 30”, and this split results in two child nodes\n",
        "where each node has mostly one class, the entropy decreases and information gain\n",
        "increases.\n",
        "Conclusion: Information Gain helps build trees that make better decisions by\n",
        "focusing on the most informative features.\n"
      ],
      "metadata": {
        "id": "TkXqFjz5Nx5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "**Applications:**\n",
        "\n",
        "● 1. Healthcare: Diagnosing diseases based on symptoms.\n",
        "\n",
        "● 2. Finance: Approving loans based on credit score, income.\n",
        "\n",
        "● 3. Marketing: Predicting customer churn or product purchase.\n",
        "\n",
        "● 4. Education: Predicting student performance.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "● Easy to understand and visualize\n",
        "\n",
        "● Can handle both numerical and categorical data\n",
        "\n",
        "● Requires little data preprocessing (no need for normalization)\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "● Prone to overfitting on noisy data\n",
        "\n",
        "● Small changes in data can change the structure drastically\n",
        "\n",
        "● Greedy approach may not lead to the optimal tree\n"
      ],
      "metadata": {
        "id": "5KK6ud90Oa7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Python Program – Load Iris Dataset and Train Decision Tree with Gini\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "random_state=1)\n",
        "# Train Decision Tree with Gini criterion\n",
        "model = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# Print results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Feature Importances:\", model.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GIC0AUwSIf0",
        "outputId": "bc8f0211-8cb1-4480-d96f-bcf04f8dd1e2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9555555555555556\n",
            "Feature Importances: [0.02146947 0.02146947 0.57196476 0.38509631]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "o51G1buBNuu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Python Program – Compare Depth-Limited vs Fully Grown Tree\n",
        "# Model with max_depth=3\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "tree_depth_3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_depth_3.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy for max_depth=3\n",
        "y_pred_depth_3 = tree_depth_3.predict(X_test)\n",
        "accuracy_depth_3 = accuracy_score(y_test, y_pred_depth_3)\n",
        "\n",
        "# Train a fully-grown Decision Tree Classifier\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy for the fully-grown tree\n",
        "y_pred_full_tree = full_tree.predict(X_test)\n",
        "accuracy_full_tree = accuracy_score(y_test, y_pred_full_tree)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy of Decision Tree with max_depth=3: {accuracy_depth_3:.2f}\")\n",
        "print(f\"Accuracy of Fully-Grown Decision Tree: {accuracy_full_tree:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AhHsTRzSa-T",
        "outputId": "be73fed7-dac0-4f3b-d334-43c850acbe03"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree with max_depth=3: 1.00\n",
            "Accuracy of Fully-Grown Decision Tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Python Program – Train Decision Tree on Boston Housing Dataset\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# Load dataset (Boston deprecated, use California Housing instead)\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "random_state=42)\n",
        "# Train model\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "# Predict and evaluate\n",
        "predictions = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(\"MSE:\", mse)\n",
        "print(\"Feature Importances:\", model.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km-uBgRESl89",
        "outputId": "7ff4dfe8-2bae-4542-eb93-e36743049e71"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.5280096503174904\n",
            "Feature Importances: [0.52345628 0.05213495 0.04941775 0.02497426 0.03220553 0.13901245\n",
            " 0.08999238 0.08880639]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdf9b319",
        "outputId": "b834794a-898b-4775-e2d3-8f5f09bae6cf"
      },
      "source": [
        "#Question 9: Python Program – Hyperparameter Tuning with GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor # Import DecisionTreeRegressor\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 4, 6]\n",
        "}\n",
        "\n",
        "# Initialize model - Use DecisionTreeRegressor for regression task\n",
        "dt = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Grid search\n",
        "grid = GridSearchCV(dt, param_grid, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best MSE:\", -grid.best_score_) # Print negative of best score for MSE"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'min_samples_split': 4}\n",
            "Best MSE: -0.6091798720196308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "**Step-by-Step Process:**\n",
        "\n",
        "1. Handling Missing Values:\n",
        "\n",
        "o Use imputation methods like SimpleImputer to fill missing values.\n",
        "\n",
        "o Mean for numerical features, most frequent or mode for categorical ones.\n",
        "\n",
        "**2. Encoding Categorical Features:**\n",
        "\n",
        "o Use OneHotEncoder or LabelEncoder based on whether features are nominal or\n",
        "ordinal.\n",
        "\n",
        "**3. Training Decision Tree Model:**\n",
        "\n",
        "o Use DecisionTreeClassifier() from scikit-learn.\n",
        "\n",
        "o Train the model on the cleaned dataset..\n",
        "\n",
        "**4. Hyperparameter Tuning:**\n",
        "\n",
        "o Use GridSearchCV to find best max_depth, min_samples_split, etc.\n",
        "\n",
        "o Helps avoid overfitting and underfitting.\n",
        "\n",
        "**5. Model Evaluation:**\n",
        "\n",
        "o Use accuracy, confusion matrix, precision-recall, and AUC-ROC to evaluate.\n",
        "\n",
        "o Perform cross-validation for reliable performance.\n",
        "\n",
        "**Business** **Value**:\n",
        "\n",
        "Helps doctors prioritize patients at risk.\n",
        "\n",
        "Enables preventive treatment by predicting disease early.\n",
        "\n",
        "Saves cost for hospitals and improves patient care.\n",
        "\n",
        "Interpretable models build trust with medical professionals.\n",
        "\n",
        "\n",
        "**Conclusion:**\n",
        "A well-tuned decision tree in healthcare can greatly improve diagnosis efficiency and\n",
        "drive data-based decision-making."
      ],
      "metadata": {
        "id": "PGSjEe6wS2hT"
      }
    }
  ]
}